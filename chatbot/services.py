import logging
import uuid
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any
from collections import defaultdict
import json
import re

import google.generativeai as genai
from qdrant_client import QdrantClient
from qdrant_client.http import models as rest
from bson import ObjectId
from pymongo import DESCENDING, ASCENDING

from config import settings
from database import get_mongo_db
from models import ChatRequest, ChatResponse, ChatHistory, SourcedAnswer, ChatContext

logger = logging.getLogger(__name__)

# --- SYSTEM PROMPT ROUTER ---
SYSTEM_PROMPT_ROUTER = """
Báº¡n lÃ  AI Query Router. Nhiá»‡m vá»¥: PhÃ¢n tÃ­ch ngá»¯ cáº£nh vÃ  cÃ¢u há»i Ä‘á»ƒ Ä‘á»‹nh tuyáº¿n.

--- INPUT DATA ---
1. Context Page: "home_page" | "list_page" | "detail_page" | "my_page"
2. Query: CÃ¢u há»i user.
3. Chat History: Lá»‹ch sá»­.

--- PHÃ‚N TÃCH ---
1. XÃC Äá»ŠNH DEPENDENCY (Sá»± phá»¥ thuá»™c):
    - "main": CÃ¢u há»i Äá»˜C Láº¬P, Ä‘áº§y Ä‘á»§ chá»§ ngá»¯/vá»‹ ngá»¯ hoáº·c má»Ÿ ra chá»§ Ä‘á» má»›i.
    - "sub": CÃ¢u há»i PHá»¤ THUá»˜C (Follow-up). Dáº¥u hiá»‡u: 
        + Äáº¡i tá»« thay tháº¿ ("nÃ³", "Ã´ng áº¥y", "bÃ i nÃ y", "danh sÃ¡ch Ä‘Ã³", ...).
        + CÃ¢u há»i ngáº¯n/cá»¥t ("tháº¿ cÃ²n tÃ¡c giáº£?", "cÃ²n ngÃ y mai?", "táº¡i sao?", ...).
        + Báº¯t Ä‘áº§u báº±ng tá»« ná»‘i ("váº­y thÃ¬", "náº¿u tháº¿", ...).
        + Tham chiáº¿u thá»© tá»± ("bÃ i 1", "cÃ¡i thá»© 2", "pháº§n Ä‘áº§u"...).
        + Tham chiáº¿u ná»™i dung ("bÃ i vá» giÃ¡ vÃ ng", "tin láº¡m phÃ¡t"...).

2. INTENT:
    - "contextual_summary": TÃ³m táº¯t, tá»•ng há»£p thÃ´ng tin tá»« Context hiá»‡n táº¡i.
    - "specific_detail": Há»i chi tiáº¿t vá» 1 Ä‘á»‘i tÆ°á»£ng cá»¥ thá»ƒ.
    - "general_search": TÃ¬m kiáº¿m má»Ÿ rá»™ng, kiáº¿n thá»©c chung.

3. TRÃCH XUáº¤T FILTERS & QUANTITY:
    - website: "vneconomy.vn" | "vnexpress.net"
    - days_ago: int (VD: "3 ngÃ y qua" -> 3)
    - topic: string (Chá»§ Ä‘á» bÃ i bÃ¡o).
    - sentiment: "positive" | "negative" | "neutral"
    - quantity: int (Sá»‘ lÆ°á»£ng bÃ i bÃ¡o user muá»‘n xá»­ lÃ½. VD: "5 bÃ i Ä‘áº§u", "top 3" -> 5, 3. Máº·c Ä‘á»‹nh null).

--- LOGIC MATRIX ---
| Page | Query keywords | -> Intent |
| :--- | :--- | :--- |
| **list_page** | Tá»« khÃ³a sá»‘ nhiá»u ("cÃ¡c bÃ i", "danh sÃ¡ch", "nhá»¯ng tin nÃ y") HOáº¶C tá»« khÃ³a tÃ³m táº¯t ("tá»•ng há»£p", "Ä‘iá»ƒm tin") | -> **contextual_summary** |
| **my_page** | Tá»« khÃ³a sá»Ÿ há»¯u/tá»•ng há»£p ("bÃ i cá»§a tÃ´i", "tÃ i liá»‡u vá»«a up", "tÃ³m táº¯t") | -> **contextual_summary** |
| **my_page** | Há»i chi tiáº¿t trong tÃ i liá»‡u Ä‘Ã£ up | -> **specific_detail** |
| **detail_page** | Há»i tÃ³m táº¯t, ná»™i dung chÃ­nh, Ä‘áº¡i Ã½ | -> **contextual_summary** |
| **detail_page** | Há»i ai, cÃ¡i gÃ¬, á»Ÿ Ä‘Ã¢u, khi nÃ o (cá»§a bÃ i bÃ¡o nÃ y) | -> **specific_detail** |
| **home_page** | Báº¥t ká»³ cÃ¢u há»i nÃ o | -> **general_search** |

OUTPUT JSON:
{
    "intent": "string",
    "dependency": "string",
    "filters": {
        "website": "string | null",
        "days_ago": "integer | null",
        "topic": "string | null",
        "sentiment": "string | null",
        "quantity": "integer | null"
    }
}
"""

SYSTEM_PROMPT_CHAT = (
    "Báº¡n lÃ  trá»£ lÃ½ AI thÃ´ng minh. Tráº£ lá»i dá»±a trÃªn thÃ´ng tin cung cáº¥p.\n"
    "LÆ¯U Ã QUAN TRá»ŒNG:\n"
    "- Náº¿u cÃ¢u há»i lÃ  cÃ¢u phá»¥ (Sub-question) hoáº·c tham chiáº¿u sá»‘ thá»© tá»± (vÃ­ dá»¥: 'bÃ i 1', 'tin Ä‘áº§u tiÃªn', 'pháº§n 1'), hÃ£y CÄ‚N Cá»¨ VÃ€O Lá»ŠCH Sá»¬ CHAT (cÃ¢u tráº£ lá»i trÆ°á»›c cá»§a Bot) Ä‘á»ƒ xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c bÃ i bÃ¡o Ä‘ang Ä‘Æ°á»£c nháº¯c Ä‘áº¿n.\n"
    "- LuÃ´n trÃ­ch dáº«n nguá»“n (Source) cho má»i thÃ´ng tin Ä‘Æ°a ra, má»—i bÃ i bÃ¡o chá»‰ trÃ­ch dáº«n nguá»“n 1 láº§n duy nháº¥t."
)

class ChatService:
    def __init__(self):
        try:
            genai.configure(api_key=settings.google_api_key)
            self.llm = genai.GenerativeModel('gemini-2.5-flash', system_instruction=SYSTEM_PROMPT_CHAT)
            self.router_llm = genai.GenerativeModel('gemini-2.5-flash', generation_config={"response_mime_type": "application/json"})
            self.embedding_model = 'models/text-embedding-004'
            self.vector_size = 384
            
            self.db = get_mongo_db()
            self.chat_histories_collection = self.db['chat_histories']
            self.articles_collection = self.db['articles'] 
            
            self.qdrant_client = QdrantClient(url=settings.qdrant_url, api_key=settings.qdrant_api_key)
            self.qdrant_collection_name = settings.qdrant_collection_name
            logger.info(f"ChatService V18.1 Ready (Updated: Added Fallback Layer 4 - Filter Relaxation).")
        except Exception as e:
            logger.error(f"Init Error: {e}")
            raise

    async def _get_chat_history(self, user_id: str, conversation_id: str) -> List[ChatHistory]:
        cursor = self.chat_histories_collection.find({
            "user_id": user_id, "conversation_id": conversation_id
        }).sort("created_at", -1).limit(5)
        history = await cursor.to_list(length=5)
        return [ChatHistory(**h) for h in history]

    async def _save_chat_history(self, user_id: str, conversation_id: str, query: str, answer: str, intent: str, dependency: str, sources: List[SourcedAnswer]):
        await self.chat_histories_collection.insert_one({
            "user_id": user_id, "conversation_id": conversation_id,
            "query": query, "answer": answer, 
            "intent": intent, "dependency": dependency,
            "sources": [s.dict() for s in sources],
            "created_at": datetime.utcnow()
        })

    async def _analyze_query(self, query: str, history: List[ChatHistory], context: ChatContext) -> Dict[str, Any]:
        try:
            chronological_history = list(reversed(history))
            history_txt = "\n".join([f"User: {h.query}\nBot: {h.answer}" for h in chronological_history])
            
            prompt = (
                f"{SYSTEM_PROMPT_ROUTER}\n\n"
                f"--- RUNTIME DATA ---\n"
                f"Context Page: {context.current_page}\n"
                f"Chat History:\n{history_txt}\n"
                f"Current Query: {query}\n"
            )
            response = await self.router_llm.generate_content_async(prompt)
            return json.loads(response.text)
        except Exception as e:
            logger.error(f"Router Error: {e}")
            return {"dependency": "main", "intent": "general_search", "filters": {}}

    async def _get_top_article_ids_from_mongo(self, search_id: str, sort_by: str, sort_order: str, limit: int) -> List[str]:
        if not search_id:
            return []
        
        sort_field = "publish_date"
        if sort_by == "sentiment":
            sort_field = "sentiment" 
        elif sort_by == "publish_date":
            sort_field = "publish_date"
            
        direction = DESCENDING if sort_order == "desc" else ASCENDING
        
        logger.info(f"ðŸ” Mongo Sort: search_id={search_id} | field={sort_field} | dir={direction} | limit={limit}")
        
        cursor = self.articles_collection.find(
            {"search_id": search_id},
            {"article_id": 1}
        ).sort(sort_field, direction).limit(limit)
        
        docs = await cursor.to_list(length=limit)
        return [doc["article_id"] for doc in docs if "article_id" in doc]

    def _build_qdrant_filters(self, base_filters: dict, extracted_filters: dict) -> Optional[rest.Filter]:
        conditions = []
        
        for key, value in base_filters.items():
            if value:
                if key == "article_id":
                    if isinstance(value, list):
                        conditions.append(rest.FieldCondition(key="article_id", match=rest.MatchAny(any=value)))
                    else:
                        conditions.append(rest.FieldCondition(key="article_id", match=rest.MatchValue(value=value)))
                elif key == "search_id":
                    conditions.append(rest.FieldCondition(key="search_id", match=rest.MatchValue(value=value)))
                elif key == "update_id":
                    conditions.append(rest.FieldCondition(key="update_id", match=rest.MatchValue(value=value)))
                else:
                    conditions.append(rest.FieldCondition(key=key, match=rest.MatchValue(value=value)))

        ai_filters = extracted_filters.get("filters", {})
        
        if ai_filters.get("website"):
            conditions.append(rest.FieldCondition(key="website", match=rest.MatchValue(value=ai_filters['website'])))
        
        if ai_filters.get("topic"):
            raw_topic = ai_filters['topic'].strip()
            topic_variations = list(set([
                raw_topic, raw_topic.lower(), raw_topic.capitalize(), raw_topic.title(), raw_topic.upper()
            ]))
            conditions.append(rest.FieldCondition(key="topic", match=rest.MatchAny(any=topic_variations)))
        
        if ai_filters.get("sentiment"):
            val = ai_filters['sentiment']
            sentiment_map = {
                "positive": "TÃ­ch cá»±c",
                "negative": "TiÃªu cá»±c",
                "neutral": "Trung tÃ­nh"
            }
            mapped_label = sentiment_map.get(val)
            if mapped_label:
                conditions.append(rest.FieldCondition(key="ai_sentiment_label", match=rest.MatchValue(value=mapped_label)))
            else:
                if val == "positive": 
                    conditions.append(rest.FieldCondition(key="sentiment", range=rest.Range(gte=0.25)))
                elif val == "negative": 
                    conditions.append(rest.FieldCondition(key="sentiment", range=rest.Range(lte=-0.25)))
        
        if ai_filters.get("days_ago") and isinstance(ai_filters["days_ago"], int):
            cutoff_date = datetime.utcnow() - timedelta(days=ai_filters["days_ago"])
            conditions.append(rest.FieldCondition(key="publish_date", range=rest.DatetimeRange(gte=cutoff_date.isoformat())))

        return rest.Filter(must=conditions) if conditions else None

    async def _search_qdrant(self, query: str, qdrant_filter: Optional[rest.Filter], limit: int = 5) -> List[rest.ScoredPoint]:
        try:
            logger.info(f"ðŸ” Qdrant Search | Limit: {limit} | Filter: {qdrant_filter}")
            embedding_result = genai.embed_content(
                model=self.embedding_model, content=query, task_type="retrieval_query", output_dimensionality=self.vector_size
            )
            
            results = self.qdrant_client.search(
                collection_name=self.qdrant_collection_name,
                query_vector=embedding_result['embedding'],
                query_filter=qdrant_filter,
                limit=limit
            )
            return results
        except Exception as e:
            logger.error(f"âŒ Qdrant Search Error: {e}")
            return []

    async def _resolve_article_id(self, input_id: str) -> str:
        if not input_id or len(input_id) != 24: return input_id
        try:
            doc = await self.articles_collection.find_one({"_id": ObjectId(input_id)}, {"article_id": 1})
            return doc["article_id"] if doc else input_id
        except: return input_id
    
    def _smart_resolve_article(self, query: str, sources: List[SourcedAnswer]) -> Optional[Tuple[str, str]]:
        if not sources:
            return None

        # 1. Check Ordinal 
        match = re.search(r'(?:bÃ i|tin|pháº§n|sá»‘|má»¥c)\s+(?:thá»©\s+)?(\d+)', query.lower())
        if match:
            try:
                val = int(match.group(1))
                idx = val - 1 if val > 0 else 0
                if 0 <= idx < len(sources):
                    logger.info(f"ðŸ”— Detected Ordinal Ref: Index {idx} -> {sources[idx].title}")
                    return sources[idx].article_id, sources[idx].title
            except:
                pass
        
        # Check text ordinal
        lower_q = query.lower()
        if "Ä‘áº§u tiÃªn" in lower_q or "thá»© nháº¥t" in lower_q: return sources[0].article_id, sources[0].title
        if ("thá»© hai" in lower_q or "thá»© 2" in lower_q) and len(sources) > 1: return sources[1].article_id, sources[1].title
        
        # 2. Check Semantic Keyword Matching
        best_match = None
        max_score = 0
        query_tokens = set(lower_q.split())
        
        for src in sources:
            title_tokens = set(src.title.lower().split())
            intersection = query_tokens.intersection(title_tokens)
            score = len(intersection)
            
            valid_match = False
            if score >= 2:
                valid_match = True
            elif score == 1:
                matched_word = list(intersection)[0]
                if len(matched_word) > 4: 
                    valid_match = True
            
            if valid_match and score > max_score:
                max_score = score
                best_match = src
        
        if best_match:
            return best_match.article_id, best_match.title

        return None

    async def handle_chat(self, request: ChatRequest) -> ChatResponse:
        conversation_id = request.conversation_id or str(uuid.uuid4())
        
        if request.context.current_page == "detail_page" and request.context.article_id:
            request.context.article_id = await self._resolve_article_id(request.context.article_id)

        history = await self._get_chat_history(request.user_id, conversation_id)
        
        analysis = await self._analyze_query(request.query, history, request.context)
        intent = analysis.get("intent", "general_search")
        dependency = analysis.get("dependency", "main")
        extracted_filters = analysis.get("filters", {})
        
        requested_quantity = extracted_filters.get("quantity")
        limit = requested_quantity if requested_quantity else 5
        
        logger.info(f"Analysis -> Intent: {intent} | Quantity: {requested_quantity}")

        search_query = request.query
        context_query_append = ""
        
        target_article_id = None
        target_article_title = None

        # [FIX 1] Logic Smart Reference chá»‰ cháº¡y khi khÃ´ng pháº£i yÃªu cáº§u sá»‘ lÆ°á»£ng nhiá»u
        # Náº¿u user há»i "3 bÃ i", ta cáº§n list, khÃ´ng pháº£i 1 bÃ i cá»¥ thá»ƒ.
        is_plural_request = requested_quantity and requested_quantity > 1

        if dependency == "sub" and not is_plural_request:
            last_bot_sources = history[0].sources if history else []
            resolved = self._smart_resolve_article(search_query, last_bot_sources)
            
            if resolved:
                target_article_id, target_article_title = resolved
                context_query_append = f"(NgÆ°á»i dÃ¹ng Ä‘ang há»i vá» bÃ i: '{target_article_title}')"
            
            last_main_query = next((h.query for h in history if getattr(h, 'dependency', 'main') == 'main'), None)
            if last_main_query:
                search_query = f"{last_main_query} {search_query}"
                if not context_query_append:
                    context_query_append = f"(Ngá»¯ cáº£nh cÅ©: '{last_main_query}')"

        base_filters = {}
        strategy = "Global Search"
        should_fallback_to_global = False

        is_list_sort_context = (
            request.context.current_page == "list_page" and 
            request.context.search_id and 
            request.context.sort_by and 
            request.context.sort_by != "relevance"
        )
        
        has_content_filters = any(
            extracted_filters.get(k) is not None
            for k in ["topic", "website", "sentiment", "days_ago"]
        )

        top_sorted_ids = []

        # --- LOGIC CHIáº¾N LÆ¯á»¢C TÃŒM KIáº¾M ---
        if target_article_id:
            base_filters = {"article_id": target_article_id}
            base_filters["type"] = "chunk" 
            strategy = f"Smart Reference (Target: {target_article_title})"
            
            if "topic" in extracted_filters: 
                del extracted_filters["topic"]
            if "website" in extracted_filters: del extracted_filters["website"]
            if "days_ago" in extracted_filters: del extracted_filters["days_ago"]
        
        elif request.context.current_page == "detail_page" and request.context.article_id:
            base_filters = {"article_id": request.context.article_id}
            strategy = "Single Page Context"

        elif request.context.current_page == "list_page" and request.context.search_id:
            if has_content_filters:
                base_filters = {"search_id": request.context.search_id}
                strategy = "Scoped Search (With Filters)"
                should_fallback_to_global = True
            elif is_list_sort_context:
                if intent == "contextual_summary" or (dependency == "sub" and intent == "specific_detail"):
                    top_sorted_ids = await self._get_top_article_ids_from_mongo(
                        request.context.search_id,
                        request.context.sort_by,
                        request.context.sort_order or "desc",
                        limit
                    )
                    if top_sorted_ids:
                        base_filters = {"article_id": top_sorted_ids}
                        strategy = f"List Sort ({request.context.sort_by}) [Sub/Summary]"
                        limit = len(top_sorted_ids) 
                    else:
                        base_filters = {"search_id": request.context.search_id}
                        strategy = "Session Context (Fallback)"
                else:
                    base_filters = {"search_id": request.context.search_id}
                    strategy = "Session Context (Filtered)"
            elif intent == "contextual_summary":
                base_filters = {"search_id": request.context.search_id}
                strategy = "Session Context (Summary)"
            elif intent == "general_search" or intent == "specific_detail":
                base_filters = {"search_id": request.context.search_id}
                strategy = "Session Context (Search)"
                should_fallback_to_global = True 
        
        elif request.context.current_page == "my_page":
            base_filters["type"] = "my-page"
            strategy = "My Page Search"
            if request.context.update_id:
                base_filters["update_id"] = request.context.update_id
                strategy = f"My Page (UpdateID: {request.context.update_id})"
            else:
                strategy = "My Page (All User Uploads)"
                
        if target_article_id:
            pass 
        elif intent == "contextual_summary" and request.context.current_page != "my_page":
            base_filters["type"] = "ai_summary"
        elif "type" not in base_filters:
            if request.context.current_page != "my_page":
                base_filters["type"] = "chunk"

        # --- THá»°C HIá»†N TÃŒM KIáº¾M ---
        # Táº§ng 1: Initial Search
        final_filter = self._build_qdrant_filters(base_filters, {"filters": extracted_filters})
        results = await self._search_qdrant(search_query, final_filter, limit=limit)

        # Táº§ng 2: Fallback 0 (Global Search)
        if not results and should_fallback_to_global:
            logger.info("âš ï¸ Scoped Search empty. Fallback to Global Search...")
            if "search_id" in base_filters: del base_filters["search_id"]
            if base_filters.get("type") == "chunk": pass 
            
            final_filter = self._build_qdrant_filters(base_filters, {"filters": extracted_filters})
            results = await self._search_qdrant(search_query, final_filter, limit=limit)
            if results: strategy = "Global Search (Fallback from Scoped)"

        # Táº§ng 3: Fallback A (Type Relaxation)
        if not results and base_filters.get("type") == "ai_summary":
            logger.info("âš ï¸ No pre-computed summaries found. Fallback to full text search...")
            if "type" in base_filters: 
                del base_filters["type"]
                if request.context.current_page == "my_page": base_filters["type"] = "my-page"
            
            final_filter = self._build_qdrant_filters(base_filters, {"filters": extracted_filters})
            results = await self._search_qdrant(search_query, final_filter, limit=limit)

        # [NEW LOGIC] Táº§ng 4: Fallback B (Filter Relaxation)
        if not results and has_content_filters and not target_article_id:
            logger.info("âš ï¸ All strict filters failed. Executing Fallback B: Filter Relaxation (Semantic Only)...")
            
            # 1. Giá»¯ láº¡i cÃ¡c bá»™ lá»c báº¯t buá»™c (System filters)
            relaxed_filters = {}
            if "search_id" in base_filters:
                relaxed_filters["search_id"] = base_filters["search_id"]
            if "update_id" in base_filters:
                relaxed_filters["update_id"] = base_filters["update_id"]
            if "type" in base_filters:
                relaxed_filters["type"] = base_filters["type"]

            # 2. Loáº¡i bá» cÃ¡c bá»™ lá»c ná»™i dung tá»« ngÆ°á»i dÃ¹ng (chá»‰ giá»¯ quantity náº¿u cáº§n)
            relaxed_ai_filters = {} 
            if "quantity" in extracted_filters:
                relaxed_ai_filters["quantity"] = extracted_filters["quantity"]

            # 3. Thá»±c hiá»‡n tÃ¬m kiáº¿m láº¡i
            final_filter_relaxed = self._build_qdrant_filters(relaxed_filters, {"filters": relaxed_ai_filters})
            results = await self._search_qdrant(search_query, final_filter_relaxed, limit=limit)
            
            if results:
                strategy = "Semantic Fallback (Filters Relaxed)"

        # --- RE-SORT RESULTS ---
        if results:
            def get_id(point):
                return point.payload.get("article_id") or point.payload.get("metadata", {}).get("article_id")
            
            if top_sorted_ids:
                id_map = {str(aid): i for i, aid in enumerate(top_sorted_ids)}
                results.sort(key=lambda x: id_map.get(str(get_id(x)), 999))
                logger.info("âœ… Re-sorted results match Mongo ID list (Sync Sources).")
            elif intent == "contextual_summary" and not target_article_id:
                results.sort(key=lambda x: x.payload.get("publish_date", ""), reverse=True)
                logger.info("âœ… Re-sorted results by Date Desc for Summary (Sync Sources).")

        context_parts = []
        sources = []
        seen = set()

        if not results:
            if intent == "contextual_summary":
                final_answer = "Hiá»‡n khÃ´ng tÃ¬m tháº¥y ná»™i dung phÃ¹ há»£p Ä‘á»ƒ tÃ³m táº¯t."
            else:
                final_answer = "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p trong danh sÃ¡ch nÃ y."
        else:
            for pt in results:
                payload = pt.payload or {}
                
                content = payload.get("summary_text") if payload.get("type") == "ai_summary" else payload.get("text", "")
                content = "\n- ".join(content) if isinstance(content, list) else str(content)
                
                title = payload.get("title", "No Title")
                aid = payload.get("article_id") or payload.get("metadata", {}).get("article_id", "unknown")
                
                publish_date = payload.get("publish_date", "N/A")
                site_categories = payload.get("site_categories", payload.get("topic", "N/A"))
                
                sentiment_label = payload.get("ai_sentiment_label", "N/A")
                sentiment_confidence = payload.get("ai_sentiment_score", "N/A")
                
                if sentiment_label == "N/A" and "sentiment" in payload:
                    sentiment_confidence = payload["sentiment"] 
                    sentiment_label = "Positive" if sentiment_confidence > 0 else "Negative"
                
                context_parts.append(
                    f"--- BÃ i: {title} ---\n"
                    f"NgÃ y Ä‘Äƒng: {publish_date}\n"
                    f"Cáº£m xÃºc AI: {sentiment_label} (Äá»™ tin cáº­y: {sentiment_confidence})\n"
                    f"Chá»§ Ä‘á»: {site_categories}\n"
                    f"Ná»™i dung:\n{content}"
                )
                
                if title not in seen:
                    sources.append(SourcedAnswer(article_id=str(aid), title=title))
                    seen.add(title)

            chat_history_str = chr(10).join([
                f"- User: {h.query}\n  Bot: {h.answer}" 
                for h in reversed(history[:2])
            ])

            # [FIX 2] Prompt Engineering: Inject Dependency & Force Data Priority
            prompt_instruction = ""
            if dependency == "main":
                prompt_instruction = (
                    "CHÃš Ã: ÄÃ¢y lÃ  cÃ¢u há»i chÃ­nh (Main Question). "
                    "HÃ£y Æ°u tiÃªn sá»­ dá»¥ng dá»¯ liá»‡u trong pháº§n 'Dá»¯ liá»‡u tÃ¬m Ä‘Æ°á»£c' bÃªn dÆ°á»›i Ä‘á»ƒ tráº£ lá»i. "
                    "Chá»‰ tham kháº£o lá»‹ch sá»­ chat náº¿u cáº§n biáº¿t phong cÃ¡ch tráº£ lá»i, KHÃ”NG dÃ¹ng dá»¯ liá»‡u cÅ© náº¿u nÃ³ khÃ´ng liÃªn quan."
                )
            else:
                prompt_instruction = "LÆ°u Ã½: ÄÃ¢y lÃ  cÃ¢u há»i phá»¥ (Sub-question), hÃ£y káº¿t há»£p ngá»¯ cáº£nh lá»‹ch sá»­ chat Ä‘á»ƒ tráº£ lá»i máº¡ch láº¡c."

            prompt = (
                f"CÃ¢u há»i ngÆ°á»i dÃ¹ng: {request.query} {context_query_append}\n"
                f"Loáº¡i cÃ¢u há»i: {dependency.upper()}\n"
                f"{prompt_instruction}\n\n"
                f"Lá»‹ch sá»­ há»™i thoáº¡i (Ä‘á»ƒ tham kháº£o ngá»¯ cáº£nh):\n"
                f"{chat_history_str}\n\n"
                f"Dá»¯ liá»‡u tÃ¬m Ä‘Æ°á»£c ({strategy}):\n{chr(10).join(context_parts)}\n\n"
                f"YÃŠU Cáº¦U: Tráº£ lá»i cÃ¢u há»i trÃªn dá»±a trÃªn dá»¯ liá»‡u cung cáº¥p. TrÃ­ch dáº«n nguá»“n rÃµ rÃ ng."
            )
            resp = await self.llm.generate_content_async(prompt)
            final_answer = resp.text

        await self._save_chat_history(request.user_id, conversation_id, request.query, final_answer, intent, dependency, sources)
        
        return ChatResponse(
            answer=final_answer, conversation_id=conversation_id, sources=sources,
            intent_detected=intent, dependency_label=dependency, strategy_used=strategy
        )




# import logging
# import uuid
# import asyncio
# from datetime import datetime, timedelta
# from typing import List, Dict, Optional, Tuple, Any
# from collections import defaultdict
# import json
# import re

# import google.generativeai as genai
# from qdrant_client import QdrantClient
# from qdrant_client.http import models as rest
# from bson import ObjectId
# from pymongo import DESCENDING, ASCENDING

# from config import settings
# from database import get_mongo_db
# from models import ChatRequest, ChatResponse, ChatHistory, SourcedAnswer, ChatContext

# logger = logging.getLogger(__name__)

# # --- SYSTEM PROMPT ROUTER ---
# SYSTEM_PROMPT_ROUTER = """
# Báº¡n lÃ  AI Query Router. Nhiá»‡m vá»¥: PhÃ¢n tÃ­ch ngá»¯ cáº£nh vÃ  cÃ¢u há»i Ä‘á»ƒ Ä‘á»‹nh tuyáº¿n.

# --- INPUT DATA ---
# 1. Context Page: "home_page" | "list_page" | "detail_page" | "my_page"
# 2. Query: CÃ¢u há»i user.
# 3. Chat History: Lá»‹ch sá»­.

# --- PHÃ‚N TÃCH ---
# 1. XÃC Äá»ŠNH DEPENDENCY (Sá»± phá»¥ thuá»™c):
#     - "main": CÃ¢u há»i Äá»˜C Láº¬P, Ä‘áº§y Ä‘á»§ chá»§ ngá»¯/vá»‹ ngá»¯ hoáº·c má»Ÿ ra chá»§ Ä‘á» má»›i.
#     - "sub": CÃ¢u há»i PHá»¤ THUá»˜C (Follow-up). Dáº¥u hiá»‡u: 
#         + Äáº¡i tá»« thay tháº¿ ("nÃ³", "Ã´ng áº¥y", "bÃ i nÃ y", "danh sÃ¡ch Ä‘Ã³", ...).
#         + CÃ¢u há»i ngáº¯n/cá»¥t ("tháº¿ cÃ²n tÃ¡c giáº£?", "cÃ²n ngÃ y mai?", "táº¡i sao?", ...).
#         + Báº¯t Ä‘áº§u báº±ng tá»« ná»‘i ("váº­y thÃ¬", "náº¿u tháº¿", ...).
#         + Tham chiáº¿u thá»© tá»± ("bÃ i 1", "cÃ¡i thá»© 2", "pháº§n Ä‘áº§u"...).
#         + Tham chiáº¿u ná»™i dung ("bÃ i vá» giÃ¡ vÃ ng", "tin láº¡m phÃ¡t"...).

# 2. INTENT:
#     - "contextual_summary": TÃ³m táº¯t, tá»•ng há»£p thÃ´ng tin tá»« Context hiá»‡n táº¡i.
#     - "specific_detail": Há»i chi tiáº¿t vá» 1 Ä‘á»‘i tÆ°á»£ng cá»¥ thá»ƒ.
#     - "general_search": TÃ¬m kiáº¿m má»Ÿ rá»™ng, kiáº¿n thá»©c chung.

# 3. TRÃCH XUáº¤T FILTERS & QUANTITY:
#     - website: "vneconomy.vn" | "vnexpress.net"
#     - days_ago: int (VD: "3 ngÃ y qua" -> 3)
#     - topic: string (Chá»§ Ä‘á» bÃ i bÃ¡o).
#     - sentiment: "positive" | "negative" | "neutral"
#     - quantity: int (Sá»‘ lÆ°á»£ng bÃ i bÃ¡o user muá»‘n xá»­ lÃ½. VD: "5 bÃ i Ä‘áº§u", "top 3" -> 5, 3. Máº·c Ä‘á»‹nh null).

# --- LOGIC MATRIX ---
# | Page | Query keywords | -> Intent |
# | :--- | :--- | :--- |
# | **list_page** | Tá»« khÃ³a sá»‘ nhiá»u ("cÃ¡c bÃ i", "danh sÃ¡ch", "nhá»¯ng tin nÃ y") HOáº¶C tá»« khÃ³a tÃ³m táº¯t ("tá»•ng há»£p", "Ä‘iá»ƒm tin") | -> **contextual_summary** |
# | **my_page** | Tá»« khÃ³a sá»Ÿ há»¯u/tá»•ng há»£p ("bÃ i cá»§a tÃ´i", "tÃ i liá»‡u vá»«a up", "tÃ³m táº¯t") | -> **contextual_summary** |
# | **my_page** | Há»i chi tiáº¿t trong tÃ i liá»‡u Ä‘Ã£ up | -> **specific_detail** |
# | **detail_page** | Há»i tÃ³m táº¯t, ná»™i dung chÃ­nh, Ä‘áº¡i Ã½ | -> **contextual_summary** |
# | **detail_page** | Há»i ai, cÃ¡i gÃ¬, á»Ÿ Ä‘Ã¢u, khi nÃ o (cá»§a bÃ i bÃ¡o nÃ y) | -> **specific_detail** |
# | **home_page** | Báº¥t ká»³ cÃ¢u há»i nÃ o | -> **general_search** |

# OUTPUT JSON:
# {
#     "intent": "string",
#     "dependency": "string",
#     "filters": {
#         "website": "string | null",
#         "days_ago": "integer | null",
#         "topic": "string | null",
#         "sentiment": "string | null",
#         "quantity": "integer | null"
#     }
# }
# """

# SYSTEM_PROMPT_CHAT = (
#     "Báº¡n lÃ  trá»£ lÃ½ AI thÃ´ng minh. Tráº£ lá»i dá»±a trÃªn thÃ´ng tin cung cáº¥p.\n"
#     "LÆ¯U Ã QUAN TRá»ŒNG:\n"
#     "- Náº¿u cÃ¢u há»i lÃ  cÃ¢u phá»¥ (Sub-question) hoáº·c tham chiáº¿u sá»‘ thá»© tá»± (vÃ­ dá»¥: 'bÃ i 1', 'tin Ä‘áº§u tiÃªn', 'pháº§n 1'), hÃ£y CÄ‚N Cá»¨ VÃ€O Lá»ŠCH Sá»¬ CHAT (cÃ¢u tráº£ lá»i trÆ°á»›c cá»§a Bot) Ä‘á»ƒ xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c bÃ i bÃ¡o Ä‘ang Ä‘Æ°á»£c nháº¯c Ä‘áº¿n.\n"
#     "- LuÃ´n trÃ­ch dáº«n nguá»“n (Source) cho má»i thÃ´ng tin Ä‘Æ°a ra, má»—i bÃ i bÃ¡o chá»‰ trÃ­ch dáº«n nguá»“n 1 láº§n duy nháº¥t."
# )

# class ChatService:
#     def __init__(self):
#         try:
#             genai.configure(api_key=settings.google_api_key)
#             self.llm = genai.GenerativeModel('gemini-2.5-flash', system_instruction=SYSTEM_PROMPT_CHAT)
#             self.router_llm = genai.GenerativeModel('gemini-2.5-flash', generation_config={"response_mime_type": "application/json"})
#             self.embedding_model = 'models/text-embedding-004'
#             self.vector_size = 384
            
#             self.db = get_mongo_db()
#             self.chat_histories_collection = self.db['chat_histories']
#             self.articles_collection = self.db['articles'] 
            
#             self.qdrant_client = QdrantClient(url=settings.qdrant_url, api_key=settings.qdrant_api_key)
#             self.qdrant_collection_name = settings.qdrant_collection_name
#             logger.info(f"ChatService V18.0 Ready (Updated: Fix Quantity Logic & Prompt Injection).")
#         except Exception as e:
#             logger.error(f"Init Error: {e}")
#             raise

#     async def _get_chat_history(self, user_id: str, conversation_id: str) -> List[ChatHistory]:
#         cursor = self.chat_histories_collection.find({
#             "user_id": user_id, "conversation_id": conversation_id
#         }).sort("created_at", -1).limit(5)
#         history = await cursor.to_list(length=5)
#         return [ChatHistory(**h) for h in history]

#     async def _save_chat_history(self, user_id: str, conversation_id: str, query: str, answer: str, intent: str, dependency: str, sources: List[SourcedAnswer]):
#         await self.chat_histories_collection.insert_one({
#             "user_id": user_id, "conversation_id": conversation_id,
#             "query": query, "answer": answer, 
#             "intent": intent, "dependency": dependency,
#             "sources": [s.dict() for s in sources],
#             "created_at": datetime.utcnow()
#         })

#     async def _analyze_query(self, query: str, history: List[ChatHistory], context: ChatContext) -> Dict[str, Any]:
#         try:
#             chronological_history = list(reversed(history))
#             history_txt = "\n".join([f"User: {h.query}\nBot: {h.answer}" for h in chronological_history])
            
#             prompt = (
#                 f"{SYSTEM_PROMPT_ROUTER}\n\n"
#                 f"--- RUNTIME DATA ---\n"
#                 f"Context Page: {context.current_page}\n"
#                 f"Chat History:\n{history_txt}\n"
#                 f"Current Query: {query}\n"
#             )
#             response = await self.router_llm.generate_content_async(prompt)
#             return json.loads(response.text)
#         except Exception as e:
#             logger.error(f"Router Error: {e}")
#             return {"dependency": "main", "intent": "general_search", "filters": {}}

#     async def _get_top_article_ids_from_mongo(self, search_id: str, sort_by: str, sort_order: str, limit: int) -> List[str]:
#         if not search_id:
#             return []
        
#         sort_field = "publish_date"
#         if sort_by == "sentiment":
#             sort_field = "sentiment" 
#         elif sort_by == "publish_date":
#             sort_field = "publish_date"
            
#         direction = DESCENDING if sort_order == "desc" else ASCENDING
        
#         logger.info(f"ðŸ” Mongo Sort: search_id={search_id} | field={sort_field} | dir={direction} | limit={limit}")
        
#         cursor = self.articles_collection.find(
#             {"search_id": search_id},
#             {"article_id": 1}
#         ).sort(sort_field, direction).limit(limit)
        
#         docs = await cursor.to_list(length=limit)
#         return [doc["article_id"] for doc in docs if "article_id" in doc]

#     def _build_qdrant_filters(self, base_filters: dict, extracted_filters: dict) -> Optional[rest.Filter]:
#         conditions = []
        
#         for key, value in base_filters.items():
#             if value:
#                 if key == "article_id":
#                     if isinstance(value, list):
#                         conditions.append(rest.FieldCondition(key="article_id", match=rest.MatchAny(any=value)))
#                     else:
#                         conditions.append(rest.FieldCondition(key="article_id", match=rest.MatchValue(value=value)))
#                 elif key == "search_id":
#                     conditions.append(rest.FieldCondition(key="search_id", match=rest.MatchValue(value=value)))
#                 elif key == "update_id":
#                     conditions.append(rest.FieldCondition(key="update_id", match=rest.MatchValue(value=value)))
#                 else:
#                     conditions.append(rest.FieldCondition(key=key, match=rest.MatchValue(value=value)))

#         ai_filters = extracted_filters.get("filters", {})
        
#         if ai_filters.get("website"):
#             conditions.append(rest.FieldCondition(key="website", match=rest.MatchValue(value=ai_filters['website'])))
        
#         if ai_filters.get("topic"):
#             raw_topic = ai_filters['topic'].strip()
#             topic_variations = list(set([
#                 raw_topic, raw_topic.lower(), raw_topic.capitalize(), raw_topic.title(), raw_topic.upper()
#             ]))
#             conditions.append(rest.FieldCondition(key="topic", match=rest.MatchAny(any=topic_variations)))
        
#         if ai_filters.get("sentiment"):
#             val = ai_filters['sentiment']
#             sentiment_map = {
#                 "positive": "TÃ­ch cá»±c",
#                 "negative": "TiÃªu cá»±c",
#                 "neutral": "Trung tÃ­nh"
#             }
#             mapped_label = sentiment_map.get(val)
#             if mapped_label:
#                 conditions.append(rest.FieldCondition(key="ai_sentiment_label", match=rest.MatchValue(value=mapped_label)))
#             else:
#                 if val == "positive": 
#                     conditions.append(rest.FieldCondition(key="sentiment", range=rest.Range(gte=0.25)))
#                 elif val == "negative": 
#                     conditions.append(rest.FieldCondition(key="sentiment", range=rest.Range(lte=-0.25)))
        
#         if ai_filters.get("days_ago") and isinstance(ai_filters["days_ago"], int):
#             cutoff_date = datetime.utcnow() - timedelta(days=ai_filters["days_ago"])
#             conditions.append(rest.FieldCondition(key="publish_date", range=rest.DatetimeRange(gte=cutoff_date.isoformat())))

#         return rest.Filter(must=conditions) if conditions else None

#     async def _search_qdrant(self, query: str, qdrant_filter: Optional[rest.Filter], limit: int = 5) -> List[rest.ScoredPoint]:
#         try:
#             logger.info(f"ðŸ” Qdrant Search | Limit: {limit} | Filter: {qdrant_filter}")
#             embedding_result = genai.embed_content(
#                 model=self.embedding_model, content=query, task_type="retrieval_query", output_dimensionality=self.vector_size
#             )
            
#             results = self.qdrant_client.search(
#                 collection_name=self.qdrant_collection_name,
#                 query_vector=embedding_result['embedding'],
#                 query_filter=qdrant_filter,
#                 limit=limit
#             )
#             return results
#         except Exception as e:
#             logger.error(f"âŒ Qdrant Search Error: {e}")
#             return []

#     async def _resolve_article_id(self, input_id: str) -> str:
#         if not input_id or len(input_id) != 24: return input_id
#         try:
#             doc = await self.articles_collection.find_one({"_id": ObjectId(input_id)}, {"article_id": 1})
#             return doc["article_id"] if doc else input_id
#         except: return input_id
    
#     def _smart_resolve_article(self, query: str, sources: List[SourcedAnswer]) -> Optional[Tuple[str, str]]:
#         if not sources:
#             return None

#         # 1. Check Ordinal 
#         match = re.search(r'(?:bÃ i|tin|pháº§n|sá»‘|má»¥c)\s+(?:thá»©\s+)?(\d+)', query.lower())
#         if match:
#             try:
#                 val = int(match.group(1))
#                 idx = val - 1 if val > 0 else 0
#                 if 0 <= idx < len(sources):
#                     logger.info(f"ðŸ”— Detected Ordinal Ref: Index {idx} -> {sources[idx].title}")
#                     return sources[idx].article_id, sources[idx].title
#             except:
#                 pass
        
#         # Check text ordinal
#         lower_q = query.lower()
#         if "Ä‘áº§u tiÃªn" in lower_q or "thá»© nháº¥t" in lower_q: return sources[0].article_id, sources[0].title
#         if ("thá»© hai" in lower_q or "thá»© 2" in lower_q) and len(sources) > 1: return sources[1].article_id, sources[1].title
        
#         # 2. Check Semantic Keyword Matching
#         best_match = None
#         max_score = 0
#         query_tokens = set(lower_q.split())
        
#         for src in sources:
#             title_tokens = set(src.title.lower().split())
#             intersection = query_tokens.intersection(title_tokens)
#             score = len(intersection)
            
#             valid_match = False
#             if score >= 2:
#                 valid_match = True
#             elif score == 1:
#                 matched_word = list(intersection)[0]
#                 if len(matched_word) > 4: 
#                     valid_match = True
            
#             if valid_match and score > max_score:
#                 max_score = score
#                 best_match = src
        
#         if best_match:
#             return best_match.article_id, best_match.title

#         return None

#     async def handle_chat(self, request: ChatRequest) -> ChatResponse:
#         conversation_id = request.conversation_id or str(uuid.uuid4())
        
#         if request.context.current_page == "detail_page" and request.context.article_id:
#             request.context.article_id = await self._resolve_article_id(request.context.article_id)

#         history = await self._get_chat_history(request.user_id, conversation_id)
        
#         analysis = await self._analyze_query(request.query, history, request.context)
#         intent = analysis.get("intent", "general_search")
#         dependency = analysis.get("dependency", "main")
#         extracted_filters = analysis.get("filters", {})
        
#         requested_quantity = extracted_filters.get("quantity")
#         limit = requested_quantity if requested_quantity else 5
        
#         logger.info(f"Analysis -> Intent: {intent} | Quantity: {requested_quantity}")

#         search_query = request.query
#         context_query_append = ""
        
#         target_article_id = None
#         target_article_title = None

#         # [FIX 1] Logic Smart Reference chá»‰ cháº¡y khi khÃ´ng pháº£i yÃªu cáº§u sá»‘ lÆ°á»£ng nhiá»u
#         # Náº¿u user há»i "3 bÃ i", ta cáº§n list, khÃ´ng pháº£i 1 bÃ i cá»¥ thá»ƒ.
#         is_plural_request = requested_quantity and requested_quantity > 1

#         if dependency == "sub" and not is_plural_request:
#             last_bot_sources = history[0].sources if history else []
#             resolved = self._smart_resolve_article(search_query, last_bot_sources)
            
#             if resolved:
#                 target_article_id, target_article_title = resolved
#                 context_query_append = f"(NgÆ°á»i dÃ¹ng Ä‘ang há»i vá» bÃ i: '{target_article_title}')"
            
#             last_main_query = next((h.query for h in history if getattr(h, 'dependency', 'main') == 'main'), None)
#             if last_main_query:
#                 search_query = f"{last_main_query} {search_query}"
#                 if not context_query_append:
#                     context_query_append = f"(Ngá»¯ cáº£nh cÅ©: '{last_main_query}')"

#         base_filters = {}
#         strategy = "Global Search"
#         should_fallback_to_global = False

#         is_list_sort_context = (
#             request.context.current_page == "list_page" and 
#             request.context.search_id and 
#             request.context.sort_by and 
#             request.context.sort_by != "relevance"
#         )
        
#         has_content_filters = any(
#             extracted_filters.get(k) is not None
#             for k in ["topic", "website", "sentiment", "days_ago"]
#         )

#         top_sorted_ids = []

#         # --- LOGIC CHIáº¾N LÆ¯á»¢C TÃŒM KIáº¾M ---
#         if target_article_id:
#             base_filters = {"article_id": target_article_id}
#             base_filters["type"] = "chunk" 
#             strategy = f"Smart Reference (Target: {target_article_title})"
#             if "topic" in extracted_filters: 
#                 del extracted_filters["topic"]
#             if "website" in extracted_filters: del extracted_filters["website"]
#             if "days_ago" in extracted_filters: del extracted_filters["days_ago"]
        
#         elif request.context.current_page == "detail_page" and request.context.article_id:
#             base_filters = {"article_id": request.context.article_id}
#             strategy = "Single Page Context"

#         elif request.context.current_page == "list_page" and request.context.search_id:
#             if has_content_filters:
#                 base_filters = {"search_id": request.context.search_id}
#                 strategy = "Scoped Search (With Filters)"
#                 should_fallback_to_global = True
#             elif is_list_sort_context:
#                 if intent == "contextual_summary" or (dependency == "sub" and intent == "specific_detail"):
#                     top_sorted_ids = await self._get_top_article_ids_from_mongo(
#                         request.context.search_id,
#                         request.context.sort_by,
#                         request.context.sort_order or "desc",
#                         limit
#                     )
#                     if top_sorted_ids:
#                         base_filters = {"article_id": top_sorted_ids}
#                         strategy = f"List Sort ({request.context.sort_by}) [Sub/Summary]"
#                         limit = len(top_sorted_ids) 
#                     else:
#                         base_filters = {"search_id": request.context.search_id}
#                         strategy = "Session Context (Fallback)"
#                 else:
#                     base_filters = {"search_id": request.context.search_id}
#                     strategy = "Session Context (Filtered)"
#             elif intent == "contextual_summary":
#                 base_filters = {"search_id": request.context.search_id}
#                 strategy = "Session Context (Summary)"
#             elif intent == "general_search" or intent == "specific_detail":
#                 base_filters = {"search_id": request.context.search_id}
#                 strategy = "Session Context (Search)"
#                 should_fallback_to_global = True 
        
#         elif request.context.current_page == "my_page":
#             base_filters["type"] = "my-page"
#             strategy = "My Page Search"
#             if request.context.update_id:
#                 base_filters["update_id"] = request.context.update_id
#                 strategy = f"My Page (UpdateID: {request.context.update_id})"
#             else:
#                 strategy = "My Page (All User Uploads)"
                
#         if target_article_id:
#             pass 
#         elif intent == "contextual_summary" and request.context.current_page != "my_page":
#             base_filters["type"] = "ai_summary"
#         elif "type" not in base_filters:
#             if request.context.current_page != "my_page":
#                 base_filters["type"] = "chunk"

#         # --- THá»°C HIá»†N TÃŒM KIáº¾M ---
#         final_filter = self._build_qdrant_filters(base_filters, {"filters": extracted_filters})
#         results = await self._search_qdrant(search_query, final_filter, limit=limit)

#         if not results and should_fallback_to_global:
#             logger.info("âš ï¸ Scoped Search empty. Fallback to Global Search...")
#             if "search_id" in base_filters: del base_filters["search_id"]
#             if base_filters.get("type") == "chunk": pass 
            
#             final_filter = self._build_qdrant_filters(base_filters, {"filters": extracted_filters})
#             results = await self._search_qdrant(search_query, final_filter, limit=limit)
#             if results: strategy = "Global Search (Fallback from Scoped)"

#         if not results and base_filters.get("type") == "ai_summary":
#             logger.info("âš ï¸ No pre-computed summaries found. Fallback to full text search...")
#             if "type" in base_filters: 
#                 del base_filters["type"]
#                 if request.context.current_page == "my_page": base_filters["type"] = "my-page"
            
#             final_filter = self._build_qdrant_filters(base_filters, {"filters": extracted_filters})
#             results = await self._search_qdrant(search_query, final_filter, limit=limit)

#         # --- RE-SORT RESULTS ---
#         if results:
#             def get_id(point):
#                 return point.payload.get("article_id") or point.payload.get("metadata", {}).get("article_id")
            
#             if top_sorted_ids:
#                 id_map = {str(aid): i for i, aid in enumerate(top_sorted_ids)}
#                 results.sort(key=lambda x: id_map.get(str(get_id(x)), 999))
#                 logger.info("âœ… Re-sorted results match Mongo ID list (Sync Sources).")
#             elif intent == "contextual_summary" and not target_article_id:
#                 results.sort(key=lambda x: x.payload.get("publish_date", ""), reverse=True)
#                 logger.info("âœ… Re-sorted results by Date Desc for Summary (Sync Sources).")

#         context_parts = []
#         sources = []
#         seen = set()

#         if not results:
#             if intent == "contextual_summary":
#                 final_answer = "Hiá»‡n khÃ´ng tÃ¬m tháº¥y ná»™i dung phÃ¹ há»£p Ä‘á»ƒ tÃ³m táº¯t."
#             else:
#                 final_answer = "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p trong danh sÃ¡ch nÃ y."
#         else:
#             for pt in results:
#                 payload = pt.payload or {}
                
#                 content = payload.get("summary_text") if payload.get("type") == "ai_summary" else payload.get("text", "")
#                 content = "\n- ".join(content) if isinstance(content, list) else str(content)
                
#                 title = payload.get("title", "No Title")
#                 aid = payload.get("article_id") or payload.get("metadata", {}).get("article_id", "unknown")
                
#                 publish_date = payload.get("publish_date", "N/A")
#                 site_categories = payload.get("site_categories", payload.get("topic", "N/A"))
                
#                 sentiment_label = payload.get("ai_sentiment_label", "N/A")
#                 sentiment_confidence = payload.get("ai_sentiment_score", "N/A")
                
#                 if sentiment_label == "N/A" and "sentiment" in payload:
#                     sentiment_confidence = payload["sentiment"] 
#                     sentiment_label = "Positive" if sentiment_confidence > 0 else "Negative"
                
#                 context_parts.append(
#                     f"--- BÃ i: {title} ---\n"
#                     f"NgÃ y Ä‘Äƒng: {publish_date}\n"
#                     f"Cáº£m xÃºc AI: {sentiment_label} (Äá»™ tin cáº­y: {sentiment_confidence})\n"
#                     f"Chá»§ Ä‘á»: {site_categories}\n"
#                     f"Ná»™i dung:\n{content}"
#                 )
                
#                 if title not in seen:
#                     sources.append(SourcedAnswer(article_id=str(aid), title=title))
#                     seen.add(title)

#             chat_history_str = chr(10).join([
#                 f"- User: {h.query}\n  Bot: {h.answer}" 
#                 for h in reversed(history[:2])
#             ])

#             # [FIX 2] Prompt Engineering: Inject Dependency & Force Data Priority
#             prompt_instruction = ""
#             if dependency == "main":
#                 prompt_instruction = (
#                     "CHÃš Ã: ÄÃ¢y lÃ  cÃ¢u há»i chÃ­nh (Main Question). "
#                     "HÃ£y Æ°u tiÃªn sá»­ dá»¥ng dá»¯ liá»‡u trong pháº§n 'Dá»¯ liá»‡u tÃ¬m Ä‘Æ°á»£c' bÃªn dÆ°á»›i Ä‘á»ƒ tráº£ lá»i. "
#                     "Chá»‰ tham kháº£o lá»‹ch sá»­ chat náº¿u cáº§n biáº¿t phong cÃ¡ch tráº£ lá»i, KHÃ”NG dÃ¹ng dá»¯ liá»‡u cÅ© náº¿u nÃ³ khÃ´ng liÃªn quan."
#                 )
#             else:
#                 prompt_instruction = "LÆ°u Ã½: ÄÃ¢y lÃ  cÃ¢u há»i phá»¥ (Sub-question), hÃ£y káº¿t há»£p ngá»¯ cáº£nh lá»‹ch sá»­ chat Ä‘á»ƒ tráº£ lá»i máº¡ch láº¡c."

#             prompt = (
#                 f"CÃ¢u há»i ngÆ°á»i dÃ¹ng: {request.query} {context_query_append}\n"
#                 f"Loáº¡i cÃ¢u há»i: {dependency.upper()}\n"
#                 f"{prompt_instruction}\n\n"
#                 f"Lá»‹ch sá»­ há»™i thoáº¡i (Ä‘á»ƒ tham kháº£o ngá»¯ cáº£nh):\n"
#                 f"{chat_history_str}\n\n"
#                 f"Dá»¯ liá»‡u tÃ¬m Ä‘Æ°á»£c ({strategy}):\n{chr(10).join(context_parts)}\n\n"
#                 f"YÃŠU Cáº¦U: Tráº£ lá»i cÃ¢u há»i trÃªn dá»±a trÃªn dá»¯ liá»‡u cung cáº¥p. TrÃ­ch dáº«n nguá»“n rÃµ rÃ ng."
#             )
#             resp = await self.llm.generate_content_async(prompt)
#             final_answer = resp.text

#         await self._save_chat_history(request.user_id, conversation_id, request.query, final_answer, intent, dependency, sources)
        
#         return ChatResponse(
#             answer=final_answer, conversation_id=conversation_id, sources=sources,
#             intent_detected=intent, dependency_label=dependency, strategy_used=strategy
#         )